{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WPertLinear(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def __init__(self, n: int, m: int) -> None:\n",
    "        self.w = torch.randn(n, m)\n",
    "        self.b = torch.randn(m)\n",
    "        self._w = torch.randn(n, m)\n",
    "        self._b = torch.randn(m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, _x):\n",
    "        self._w = torch.randn_like(self._w).normal_()\n",
    "        self._b = torch.randn_like(self._b).normal_()\n",
    "        y = torch.matmul(x, self.w) + self.b\n",
    "        _y = torch.matmul(x, self._w) + torch.matmul(_x, self.w) + self._b\n",
    "        return y, _y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perturb(self, _objective):\n",
    "        self.w.grad = self._w * ((_objective > 0) * 2.0 - 1)\n",
    "        self.b.grad = self._b * ((_objective > 0) * 2.0 - 1)\n",
    "\n",
    "class WPertReLU(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._m = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, _x):\n",
    "        self._m = (x >= 0) * 1.0\n",
    "        y = x.relu()\n",
    "        return y, self._m * _x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perturb(self, _objective):\n",
    "        self.y.grad = self._m * ((_objective > 0) * 2.0 - 1)\n",
    "\n",
    "class WPertConv2d(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def __init__(self, kernel_size: int, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel = torch.randn(out_channels, in_channels, *kernel_size)\n",
    "        self._kernel = torch.randn_like(self.kernel)\n",
    "        self.bias = torch.randn(out_channels)\n",
    "        self._bias = torch.randn_like(self.bias)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, _x):\n",
    "        self._kernel = torch.rand_like(self._kernel).normal_()\n",
    "        self._bias = torch.rand_like(self._bias).normal_()\n",
    "        y = F.conv2d(x, self.kernel, self.bias)\n",
    "        _y = F.conv2d(_x, self.kernel) + F.conv2d(x, self._kernel) + self._bias\n",
    "        return y, _y\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def perturb(self, _objective):\n",
    "        self.kernel.grad = self._kernel * ((_objective > 0) * 2.0 - 1)\n",
    "        self.bias.grad = self._bias * ((_objective > 0) * 2.0 - 1)\n",
    "\n",
    "class WPertSoftmax(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self._m = 0.0\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, _x):\n",
    "        # \\sum_j!=i (e^{x_j} / (\\sum_k e^{x_k})) + e^{x_i} / (\\sum_k e^{x_k})\n",
    "        # (\\sum_j -e^{x_j+x_i}) / (\\sum_k e^{x_k})^2 + (\\sum_k e^{x_k+x_i}) / (\\sum_k e^{x_k})^2\n",
    "        # self._m = \n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def perturb(self, _objective):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:14:58) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce06d59e2e7271c8b3b397e7651e310dc24c8ee6b2b2b651c89e9cca3febad42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
